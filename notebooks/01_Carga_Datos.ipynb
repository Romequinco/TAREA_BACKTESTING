{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 1: Carga de Datos\n",
        "\n",
        "## Objetivo\n",
        "Este notebook se encarga exclusivamente de descargar y almacenar los datos necesarios para toda la práctica.\n",
        "\n",
        "## Estructura\n",
        "1. Imports y Configuración\n",
        "2. Verificación del Archivo desde Google Drive\n",
        "3. Carga del Dataset\n",
        "4. Separación del Benchmark\n",
        "5. Guardado del Dataset Completo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports y Configuración"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Librerías importadas correctamente\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Configuración de pandas para mejor visualización\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.4f' % x)\n",
        "\n",
        "# Constantes\n",
        "DATA_RAW_DIR = '../datos/raw'\n",
        "BENCHMARK_TICKER = 'SPY'\n",
        "START_DATE = '2015-01-01'\n",
        "\n",
        "print('Librerías importadas correctamente')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Verificación del Archivo desde Google Drive\n",
        "\n",
        "**Importante:** El archivo debe descargarse manualmente desde Google Drive y colocarse en la carpeta `datos/raw/` con el nombre `historical_prices.parquet`.\n",
        "\n",
        "Enlace: https://drive.google.com/file/d/1nvubXdAu0EONlrP_yrURZbnPhBQ-uDaB/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Archivo de datos no encontrado. Por favor, asegúrate de que el archivo parquet esté disponible.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m downloaded_file_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     23\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mArchivo de datos no encontrado. \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     24\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mPor favor, asegúrate de que el archivo parquet esté disponible.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     25\u001b[39m     )\n",
            "\u001b[31mFileNotFoundError\u001b[39m: Archivo de datos no encontrado. Por favor, asegúrate de que el archivo parquet esté disponible."
          ]
        }
      ],
      "source": [
        "# Ruta del archivo de datos\n",
        "# Buscar el archivo en el directorio raíz o en datos/raw\n",
        "possible_paths = [\n",
        "    '../sp500_history.parquet',\n",
        "    'sp500_history.parquet',\n",
        "    '../datos/raw/historical_prices.parquet',\n",
        "    '../datos/raw/sp500_history.parquet',\n",
        "]\n",
        "\n",
        "downloaded_file_path = None\n",
        "for path in possible_paths:\n",
        "    try:\n",
        "        test_df = pd.read_parquet(path)\n",
        "        downloaded_file_path = path\n",
        "        print(f'Archivo encontrado en: {downloaded_file_path}')\n",
        "        del test_df\n",
        "        break\n",
        "    except (FileNotFoundError, OSError, Exception):\n",
        "        continue\n",
        "\n",
        "if downloaded_file_path is None:\n",
        "    raise FileNotFoundError(\n",
        "        'Archivo de datos no encontrado. '\n",
        "        'Por favor, asegúrate de que el archivo parquet esté disponible.'\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Carga del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_parquet_dataset(file_path):\n",
        "    \"\"\"\n",
        "    Carga un dataset desde un archivo Parquet.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        Ruta al archivo Parquet\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con los datos cargados\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f'Cargando dataset desde: {file_path}')\n",
        "        df = pd.read_parquet(file_path)\n",
        "        print('Dataset cargado correctamente')\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f'Error al cargar el archivo: {str(e)}')\n",
        "        raise\n",
        "\n",
        "\n",
        "# Cargar el dataset completo\n",
        "dataset = load_parquet_dataset(downloaded_file_path)\n",
        "\n",
        "# Verificar estructura básica\n",
        "print('\\n=== INFORMACIÓN DEL DATASET ===')\n",
        "print(f'Shape del dataset: {dataset.shape}')\n",
        "print(f'Columnas: {len(dataset.columns)}')\n",
        "print(f'Filas: {len(dataset)}')\n",
        "\n",
        "# Verificar índice de fechas\n",
        "if not isinstance(dataset.index, pd.DatetimeIndex):\n",
        "    print('Convirtiendo índice a DatetimeIndex...')\n",
        "    dataset.index = pd.to_datetime(dataset.index)\n",
        "\n",
        "# Ordenar por fecha\n",
        "dataset = dataset.sort_index()\n",
        "\n",
        "# Mostrar rango de fechas\n",
        "print(f'\\nRango de fechas disponible:')\n",
        "print(f'  Fecha inicio: {dataset.index.min()}')\n",
        "print(f'  Fecha fin: {dataset.index.max()}')\n",
        "\n",
        "# Contar tickers únicos\n",
        "if isinstance(dataset.columns, pd.MultiIndex):\n",
        "    tickers = dataset.columns.get_level_values(0).unique()\n",
        "else:\n",
        "    tickers = dataset.columns.unique()\n",
        "\n",
        "print(f'\\nNúmero de tickers únicos: {len(tickers)}')\n",
        "\n",
        "# Mostrar primeras filas para verificación\n",
        "print('\\n=== PRIMERAS FILAS DEL DATASET ===')\n",
        "print(dataset.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Separación del Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_benchmark_data(df, benchmark_ticker):\n",
        "    \"\"\"\n",
        "    Extrae los datos del benchmark del dataset completo.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        Dataset completo con todos los activos\n",
        "    benchmark_ticker : str\n",
        "        Símbolo del ticker del benchmark (ej: 'SPY')\n",
        "    \n",
        "    Retorna:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        DataFrame con datos del benchmark\n",
        "    \"\"\"\n",
        "    # Verificar si el dataset tiene MultiIndex en columnas\n",
        "    if isinstance(df.columns, pd.MultiIndex):\n",
        "        # Buscar el ticker en el primer nivel\n",
        "        if benchmark_ticker in df.columns.get_level_values(0):\n",
        "            benchmark_data = df[benchmark_ticker]\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'Benchmark {benchmark_ticker} no encontrado en el dataset'\n",
        "            )\n",
        "    else:\n",
        "        # Columnas simples\n",
        "        if benchmark_ticker in df.columns:\n",
        "            benchmark_data = df[[benchmark_ticker]]\n",
        "        else:\n",
        "            # Intentar buscar con diferentes variaciones\n",
        "            possible_names = [\n",
        "                benchmark_ticker,\n",
        "                benchmark_ticker.upper(),\n",
        "                benchmark_ticker.lower(),\n",
        "            ]\n",
        "            found = False\n",
        "            for name in possible_names:\n",
        "                if name in df.columns:\n",
        "                    benchmark_data = df[[name]]\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                raise ValueError(\n",
        "                    f'Benchmark {benchmark_ticker} no encontrado en el dataset'\n",
        "                )\n",
        "    \n",
        "    print(f'Benchmark {benchmark_ticker} extraído correctamente')\n",
        "    print(f'Shape del benchmark: {benchmark_data.shape}')\n",
        "    return benchmark_data\n",
        "\n",
        "\n",
        "# Extraer datos del benchmark SPY\n",
        "spy_data = extract_benchmark_data(dataset, BENCHMARK_TICKER)\n",
        "\n",
        "# Guardar benchmark\n",
        "spy_output_path = f'{DATA_RAW_DIR}/spy_data.parquet'\n",
        "try:\n",
        "    spy_data.to_parquet(spy_output_path, engine='pyarrow')\n",
        "    print(f'Benchmark guardado en: {spy_output_path}')\n",
        "except (OSError, FileNotFoundError):\n",
        "    print(f'Error: No se pudo crear el directorio. '\n",
        "          f'Asegúrate de que {DATA_RAW_DIR} existe.')\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Guardado del Dataset Completo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_dataset_parquet(df, output_path):\n",
        "    \"\"\"\n",
        "    Guarda un DataFrame en formato Parquet.\n",
        "    \n",
        "    Parámetros:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        DataFrame a guardar\n",
        "    output_path : str\n",
        "        Ruta donde guardar el archivo\n",
        "    \"\"\"\n",
        "    print(f'Guardando dataset en: {output_path}')\n",
        "    df.to_parquet(output_path, engine='pyarrow')\n",
        "    print(f'Dataset guardado exitosamente')\n",
        "\n",
        "\n",
        "# Guardar dataset completo de tickers\n",
        "tickers_output_path = f'{DATA_RAW_DIR}/tickers_data.parquet'\n",
        "try:\n",
        "    save_dataset_parquet(dataset, tickers_output_path)\n",
        "except (OSError, FileNotFoundError):\n",
        "    print(f'Error: No se pudo crear el directorio. '\n",
        "          f'Asegúrate de que {DATA_RAW_DIR} existe.')\n",
        "    raise\n",
        "\n",
        "# Resumen final\n",
        "print('\\n=== RESUMEN DEL NOTEBOOK 1 ===')\n",
        "print('Proceso completado exitosamente')\n",
        "print(f'\\nArchivos generados:')\n",
        "print(f'  1. {downloaded_file_path}')\n",
        "print(f'  2. {spy_output_path}')\n",
        "print(f'  3. {tickers_output_path}')\n",
        "print(f'\\nInformación del dataset:')\n",
        "print(f'  - Total de tickers: {len(tickers)}')\n",
        "print(f'  - Rango de fechas: {dataset.index.min()} a {dataset.index.max()}')\n",
        "print(f'  - Total de registros: {len(dataset)}')\n",
        "print('\\nNotebook 1 completado correctamente')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
